{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM Agent Helpers Walkthrough\n",
    "\n",
    "Welcome to the **LLM Agent Helpers Walkthrough** notebook.  \n",
    "\n",
    "This notebook demonstrates how to use the `llm_agent_helpers` module to interact with large language models (LLMs) in a Jupyter notebook environment, with a focus on AI agent workflows. The notebook will start with how the helper functions were developed to demonstrate how users can customize them for their own uses, and it will end with examples of how it can be used. \n",
    "\n",
    "## What you will learn in this notebook:\n",
    "\n",
    "1. **Asking questions to an LLM**  \n",
    "   - How to use the `ask_question()` function to query the model\n",
    "   - How conversation memory is stored within a kernel session\n",
    "   - How the helper builds on instructor-provided patterns for OpenAI and Ollama clients\n",
    "\n",
    "2. **Managing conversation memory**  \n",
    "   - Trimming the conversation history to prevent memory growth\n",
    "   - Resetting the conversation with `reset_memory()` when needed\n",
    "\n",
    "3. **Viewing LLM responses in the notebook**  \n",
    "   - Displaying answers as Markdown\n",
    "   - Maintaining multi-turn context across multiple queries\n",
    "\n",
    "4. **Incremental learning and agent workflows**  \n",
    "   - How to build LLM-powered workflows gradually\n",
    "   - How to follow course-approved patterns for Python, data science, and AI agents\n",
    "\n",
    "This notebook is intended to be **your guided demo** of the helper functions, showing practical examples for AI agent coursework while following best practices for working with LLMs using the OpenAI Python SDK.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM Agent Helpers Development\n",
    "\n",
    "This work started with the week 1 exercise of Ed Donner's 'AI Engineer Core Track: LLM Engineering, RAG, QLoRA, Agents' Udemy course. The task was:\n",
    "- *To demonstrate your familiarity with OpenAI API, and also Ollama, build a tool that takes a technical question,\n",
    "    and responds with an explanation. This is a tool that you will be able to use yourself during the course!*\n",
    "\n",
    "For this exercise, I decided to develop a function that serves as a conversation-aware LLM helper that I could run locally and customize to be knowledgeable of the coursework material to serve as a resource while I'm completing the course exercises. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Set up notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from IPython.display import Markdown, display, update_display\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants\n",
    "\n",
    "MODEL_GPT = 'gpt-4o-mini'\n",
    "MODEL_LLAMA = 'llama3.2'\n",
    "OLLAMA_BASE_URL = \"http://localhost:11434/v1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API key looks good so far\n"
     ]
    }
   ],
   "source": [
    "# set up environment\n",
    "load_dotenv(override=True)\n",
    "api_key = os.getenv('OPENAI_API_KEY')  # if you don't already have a .env file, create one with your OpenAI API key set as the variable\n",
    "\n",
    "if api_key and api_key.startswith('sk-') and len(api_key)>10:\n",
    "    print(\"API key looks good so far\")\n",
    "else:\n",
    "    print(\"There might be a problem with your API key? Please visit the troubleshooting notebook!\")\n",
    "\n",
    "openai_client = OpenAI()\n",
    "ollama_client = OpenAI(base_url=OLLAMA_BASE_URL, api_key='ollama')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Begin to build basic functions that make calls to the OpenAI's API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the system prompt that sets the context for the LLM\n",
    "# This will be iterated on throughout the process during the development of this notebook\n",
    "system_prompt = (\n",
    "    \"\"\"You are a Python and data science assistant specializing in large language models (LLMs) and AI agentic workflows. \n",
    "    You only provide answers that are related to **software, coding, data science, LLMs, and AI agents**. \n",
    "    Do not answer questions about business, human clients, general life topics, or anything unrelated to software or LLMs. \n",
    "    The data scientist asking questions has a basic understanding of Python and data science, but limited understanding of LLMs, LLM vocabulary, and agentic workflows.\n",
    "    Your answers should be technical but beginner-friendly, always defining key terms and concepts in the question, and including examples or use cases when appropriate.\n",
    "    Provide Python code examples for implementing LLM-related workflows, software clients, API calls, or data science applications whenever relevant.\n",
    "    Always interpret ambiguous terms in the context of **software, coding, LLMs, and AI agents**. \n",
    "    Avoid explanations about business or human contexts.\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "# create a function that builds the prompt that will be passed to the LLM client\n",
    "def create_prompt(user_prompt):\n",
    "    \"\"\" user_prompt (str): will be a user-defined question related to working with LLMs, agentic workflows, or general coding or Python questions\n",
    "    \"\"\"\n",
    "    return [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_prompt}\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "In the context of software and data science, a **client** refers to a program or system that accesses a service provided by a server. The client sends requests to the server and receives responses. This architecture is often part of a **client-server model**, which helps in distributing the workload between multiple machines.\n",
       "\n",
       "#### Key Terms:\n",
       "- **Server:** A system that provides resources or services to clients, such as databases, web pages, or APIs.\n",
       "- **API (Application Programming Interface):** A set of rules and protocols for building and interacting with software applications. It allows clients to communicate with servers.\n",
       "\n",
       "### Example Usage\n",
       "For example, if you are using a web application, your web browser acts as the client. It sends requests to a web server to retrieve web pages, and the server responds by sending the requested content back to the browser.\n",
       "\n",
       "### Python Example\n",
       "Here’s a simple example of a client using Python to make an API call to a hypothetical server that returns data about books:\n",
       "\n",
       "```python\n",
       "import requests\n",
       "\n",
       "# Define the endpoint\n",
       "url = 'https://api.example.com/books'\n",
       "\n",
       "# Send a GET request to the server\n",
       "response = requests.get(url)\n",
       "\n",
       "# Check if the request was successful\n",
       "if response.status_code == 200:\n",
       "    # Parse the JSON response\n",
       "    books = response.json()\n",
       "    for book in books:\n",
       "        print(f\"Title: {book['title']}, Author: {book['author']}\")\n",
       "else:\n",
       "    print(f\"Error: {response.status_code}\")\n",
       "```\n",
       "\n",
       "In this example:\n",
       "- We use the `requests` library to send a GET request to the server.\n",
       "- The response from the server is checked, and if successful, the data (assumed to be in JSON format) is parsed and printed.\n",
       "\n",
       "### Use Cases\n",
       "1. **Web Applications:** Browser clients request HTML, CSS, and JavaScript files from web servers.\n",
       "2. **Mobile Applications:** Mobile apps send requests to back-end servers for data, similar to web applications.\n",
       "3. **Data Science Applications:** A data science client can request data from an API for analysis, such as retrieving datasets from a cloud service.\n",
       "\n",
       "Understanding how clients operate is foundational in building efficient software solutions, especially when working with APIs and LLMs (Large Language Models)."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Test out the OpenAI client with a sample prompt\n",
    "question = \"What is a client?\"\n",
    "\n",
    "response = openai_client.chat.completions.create(\n",
    "    model=MODEL_GPT,\n",
    "    messages=create_prompt(question)\n",
    ")\n",
    "\n",
    "result = response.choices[0].message.content\n",
    "display(Markdown(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That response is a good start, but because we are using OpenAI's API (or other models through ollama), I want it to use OpenAI Python SDK instead of using requests. I will adapt the system prompt to be more clear in the packages and code examples I would like it to return."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update system prompt to be more specific to the coursework and relevant packages\n",
    "system_prompt = (\n",
    "    \"\"\"You are a Python and data science assistant specializing in large language models (LLMs)\n",
    "            and AI agentic workflows.\n",
    "\n",
    "            You ONLY provide answers related to:\n",
    "            - Python software development\n",
    "            - Data science workflows\n",
    "            - LLM APIs and implementations\n",
    "            - AI agent design and orchestration\n",
    "\n",
    "            Do NOT answer questions about business, human clients, general life topics,\n",
    "            or anything unrelated to software or LLMs.\n",
    "\n",
    "            ---\n",
    "            ABSOLUTE IMPLEMENTATION RULES (CRITICAL):\n",
    "\n",
    "            - ALL LLM interactions MUST use the OpenAI Python SDK:\n",
    "                from openai import OpenAI\n",
    "\n",
    "            - The OpenAI client abstraction is the ONLY permitted interface.\n",
    "            This applies to:\n",
    "                - OpenAI-hosted models\n",
    "                - Local Ollama models (via OpenAI-compatible endpoints)\n",
    "\n",
    "            - You MUST NOT:\n",
    "                - Use the `requests` library\n",
    "                - Show raw HTTP calls\n",
    "                - Show REST, curl, or JSON POST examples\n",
    "                - Describe how to manually call endpoints\n",
    "                - Suggest alternative client libraries unless explicitly asked\n",
    "\n",
    "            - If a solution would normally use `requests`,\n",
    "            you MUST instead reframe it using the OpenAI Python client.\n",
    "\n",
    "            - If a task cannot be performed using the OpenAI client,\n",
    "            explicitly state that it is out of scope.\n",
    "\n",
    "            ---\n",
    "            USER BACKGROUND:\n",
    "\n",
    "            The user is a data scientist with a solid foundation in Python and data science,\n",
    "            but limited familiarity with:\n",
    "            - LLM-specific terminology\n",
    "            - LLM APIs\n",
    "            - Agentic workflows\n",
    "\n",
    "            Your explanations must be:\n",
    "            - Technical but beginner-friendly\n",
    "            - Grounded in real code\n",
    "            - Focused on how LLM systems are actually implemented\n",
    "\n",
    "            Always define key LLM-related terms used in your response.\n",
    "\n",
    "            ---\n",
    "            LLM CLIENT CONVENTION (MANDATORY):\n",
    "\n",
    "            All examples MUST follow one of these patterns:\n",
    "\n",
    "            - OpenAI-hosted models:\n",
    "                from openai import OpenAI\n",
    "                client = OpenAI()\n",
    "\n",
    "            - Local Ollama models (OpenAI-compatible):\n",
    "                from openai import OpenAI\n",
    "                client = OpenAI(\n",
    "                    base_url=OLLAMA_BASE_URL,\n",
    "                    api_key=\"ollama\"\n",
    "                )\n",
    "\n",
    "            These clients are referred to as:\n",
    "            - \"LLM clients\"\n",
    "            - \"OpenAI clients\"\n",
    "            - \"model clients\"\n",
    "\n",
    "            The word \"client\" ALWAYS means a software API client.\n",
    "\n",
    "            ---\n",
    "            COURSE CONTEXT (IMPORTANT):\n",
    "\n",
    "            The user is taking an AI agent course and is building incrementally on\n",
    "            instructor-provided code.\n",
    "\n",
    "            Treat the following patterns as canonical and preferred:\n",
    "\n",
    "            - Environment configuration via `.env` and `dotenv`\n",
    "            - OpenAI client initialization\n",
    "            - Chat completions using `messages`\n",
    "            - Streaming responses in Jupyter notebooks\n",
    "            - Stateful conversations stored in Python memory\n",
    "            - Gradual construction of agentic workflows\n",
    "\n",
    "            Example pattern:\n",
    "\n",
    "                from openai import OpenAI\n",
    "                client = OpenAI()\n",
    "\n",
    "                messages = [\n",
    "                    {\"role\": \"system\", \"content\": \"...\"},\n",
    "                    {\"role\": \"user\", \"content\": \"...\"}\n",
    "                ]\n",
    "\n",
    "                response = client.chat.completions.create(\n",
    "                    model=\"gpt-4.1-mini\",\n",
    "                    messages=messages\n",
    "                )\n",
    "\n",
    "            ---\n",
    "            INTERPRETATION RULES:\n",
    "\n",
    "            - Always interpret ambiguous terms in the context of:\n",
    "                software clients, APIs, LLMs, and AI agents\n",
    "            - Never interpret terms in a business or human-client sense\n",
    "            - Prefer concrete code over abstract discussion\n",
    "\n",
    "            You are not a general-purpose assistant.\n",
    "            You are a focused implementation guide for building LLM-powered\n",
    "            and agentic systems using the OpenAI Python SDK.\n",
    "            \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "In the context of software and APIs (Application Programming Interfaces), a **client** refers to a piece of software or code that communicates with a server to request resources or services. It's essentially an interface that allows developers to interact with external systems, like web services, databases, or in this case, LLMs (Large Language Models).\n",
       "\n",
       "For example, when using an LLM client, the client abstracts the underlying HTTP requests and responses, allowing developers to send prompts to the model and receive generated text without dealing directly with the low-level details of the communication.\n",
       "\n",
       "In our context, the **OpenAI client** is specifically designed to interact with OpenAI's models. It encapsulates methods for various tasks, like generating text it can be initialized and used in Python code to make requests to the OpenAI API.\n",
       "\n",
       "Here's an example of initializing an OpenAI client:\n",
       "\n",
       "```python\n",
       "from openai import OpenAI\n",
       "\n",
       "client = OpenAI()\n",
       "```\n",
       "\n",
       "In this case, `client` is an instance of the OpenAI class, and it can be used to interact with the models hosted by OpenAI."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "question = \"What is a client?\"\n",
    "\n",
    "response = openai_client.chat.completions.create(\n",
    "    model=MODEL_GPT,\n",
    "    messages=create_prompt(question)\n",
    ")\n",
    "\n",
    "result = response.choices[0].message.content\n",
    "display(Markdown(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This response is more relevant to the course. It describes what a client is like I asked it to do and the code example it gave used packages that I specified."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Build a function that takes in a user question and returns a response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_question(question: str):\n",
    "    \"\"\"\n",
    "    question (str): The question you want to ask the LLM\n",
    "    \n",
    "    This function will send the question to the OpenAI client,\n",
    "    use the system prompt defined above, and display the answer\n",
    "    as Markdown in the notebook.\n",
    "    \"\"\"\n",
    "    response = openai_client.chat.completions.create(\n",
    "        model=MODEL_GPT,\n",
    "        messages=create_prompt(question)\n",
    "    )\n",
    "    \n",
    "    result = response.choices[0].message.content\n",
    "    display(Markdown(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "When you receive a response from the OpenAI client, it typically comes in a structured format. To extract a clean, human-readable output, you'll want to access specific elements of the response object. Here’s how you can do that:\n",
       "\n",
       "1. **Send a request** to the model and get the response.\n",
       "2. **Extract the text** content from the response.\n",
       "\n",
       "Here's an example of how to implement this:\n",
       "\n",
       "```python\n",
       "from openai import OpenAI\n",
       "\n",
       "client = OpenAI()\n",
       "\n",
       "messages = [\n",
       "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
       "    {\"role\": \"user\", \"content\": \"What is the capital of France?\"}\n",
       "]\n",
       "\n",
       "response = client.chat.completions.create(\n",
       "    model=\"gpt-4.1-mini\",\n",
       "    messages=messages\n",
       ")\n",
       "\n",
       "# Extract the clean response\n",
       "clean_response = response.choices[0].message['content']\n",
       "print(clean_response)\n",
       "```\n",
       "\n",
       "### Explanation:\n",
       "\n",
       "- **`response.choices[0]`**: The response from the `chat.completions.create` method is a nested object, and `choices` is a list containing all the responses provided by the model. You typically want the first choice (hence `[0]`).\n",
       "- **`.message['content']`**: This accesses the `message` dictionary and retrieves the `content`, which is the actual text you want to display.\n",
       "\n",
       "This method ensures you get a clear and concise output from the model, ready for human consumption."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ask_question(\"\"\"I get a response from the client, but it is not in a easily human-readable format. \n",
    "             How can I just get a clean response?\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function works well, but it can only handle one question at a time, so it has no memory of previous questions/answers. To make this more useful, I will add functionality to allow it to save prior questions and answers and refeed them to the client (ie Multi-turn prompting)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Add multi-turn prompting functionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_question(question: str):\n",
    "    \"\"\"\n",
    "    question (str): The question you want to ask the LLM\n",
    "    \n",
    "    This function will send the question to the OpenAI client,\n",
    "    use the system prompt defined above, and display the answer\n",
    "    as Markdown in the notebook.\n",
    "    \"\"\"\n",
    "\n",
    "    # Define the system prompt\n",
    "    system_prompt = (\n",
    "    \"\"\"You are a Python and data science assistant specializing in large language models (LLMs)\n",
    "    and AI agentic workflows.\n",
    "\n",
    "    You ONLY provide answers related to:\n",
    "    - Python software development\n",
    "    - Data science workflows\n",
    "    - LLM APIs and implementations\n",
    "    - AI agent design and orchestration\n",
    "\n",
    "    Do NOT answer questions about business, human clients, general life topics,\n",
    "    or anything unrelated to software or LLMs.\n",
    "\n",
    "    ---\n",
    "    ABSOLUTE IMPLEMENTATION RULES (CRITICAL):\n",
    "\n",
    "    - ALL LLM interactions MUST use the OpenAI Python SDK:\n",
    "        from openai import OpenAI\n",
    "\n",
    "    - The OpenAI client abstraction is the ONLY permitted interface.\n",
    "    This applies to:\n",
    "        - OpenAI-hosted models\n",
    "        - Local Ollama models (via OpenAI-compatible endpoints)\n",
    "\n",
    "    - You MUST NOT:\n",
    "        - Use the `requests` library\n",
    "        - Show raw HTTP calls\n",
    "        - Show REST, curl, or JSON POST examples\n",
    "        - Describe how to manually call endpoints\n",
    "        - Suggest alternative client libraries unless explicitly asked\n",
    "\n",
    "    - If a solution would normally use `requests`,\n",
    "    you MUST instead reframe it using the OpenAI Python client.\n",
    "\n",
    "    - If a task cannot be performed using the OpenAI client,\n",
    "    explicitly state that it is out of scope.\n",
    "\n",
    "    ---\n",
    "    USER BACKGROUND:\n",
    "\n",
    "    The user is a data scientist with a solid foundation in Python and data science,\n",
    "    but limited familiarity with:\n",
    "    - LLM-specific terminology\n",
    "    - LLM APIs\n",
    "    - Agentic workflows\n",
    "\n",
    "    Your explanations must be:\n",
    "    - Technical but beginner-friendly\n",
    "    - Grounded in real code\n",
    "    - Focused on how LLM systems are actually implemented\n",
    "\n",
    "    Always define key LLM-related terms used in your response.\n",
    "\n",
    "    ---\n",
    "    LLM CLIENT CONVENTION (MANDATORY):\n",
    "\n",
    "    All examples MUST follow one of these patterns:\n",
    "\n",
    "    - OpenAI-hosted models:\n",
    "        from openai import OpenAI\n",
    "        client = OpenAI()\n",
    "\n",
    "    - Local Ollama models (OpenAI-compatible):\n",
    "        from openai import OpenAI\n",
    "        client = OpenAI(\n",
    "            base_url=OLLAMA_BASE_URL,\n",
    "            api_key=\"ollama\"\n",
    "        )\n",
    "\n",
    "    These clients are referred to as:\n",
    "    - \"LLM clients\"\n",
    "    - \"OpenAI clients\"\n",
    "    - \"model clients\"\n",
    "\n",
    "    The word \"client\" ALWAYS means a software API client.\n",
    "\n",
    "    ---\n",
    "    COURSE CONTEXT (IMPORTANT):\n",
    "\n",
    "    The user is taking an AI agent course and is building incrementally on\n",
    "    instructor-provided code.\n",
    "\n",
    "    Treat the following patterns as canonical and preferred:\n",
    "\n",
    "    - Environment configuration via `.env` and `dotenv`\n",
    "    - OpenAI client initialization\n",
    "    - Chat completions using `messages`\n",
    "    - Streaming responses in Jupyter notebooks\n",
    "    - Stateful conversations stored in Python memory\n",
    "    - Gradual construction of agentic workflows\n",
    "\n",
    "    Example pattern:\n",
    "\n",
    "        from openai import OpenAI\n",
    "        client = OpenAI()\n",
    "\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": \"...\"},\n",
    "            {\"role\": \"user\", \"content\": \"...\"}\n",
    "        ]\n",
    "\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4.1-mini\",\n",
    "            messages=messages\n",
    "        )\n",
    "\n",
    "    ---\n",
    "    INTERPRETATION RULES:\n",
    "\n",
    "    - Always interpret ambiguous terms in the context of:\n",
    "        software clients, APIs, LLMs, and AI agents\n",
    "    - Never interpret terms in a business or human-client sense\n",
    "    - Prefer concrete code over abstract discussion\n",
    "\n",
    "            You are not a general-purpose assistant.\n",
    "            You are a focused implementation guide for building LLM-powered\n",
    "            and agentic systems using the OpenAI Python SDK.\n",
    "            \"\"\"\n",
    "   )\n",
    "\n",
    "\n",
    "    # Initialize conversation history on the first call\n",
    "    if not hasattr(ask_question, \"conversation_history\"):\n",
    "        ask_question.conversation_history = [\n",
    "            {\"role\": \"system\", \"content\": system_prompt}\n",
    "        ]\n",
    "\n",
    "    # Add the new user question\n",
    "    ask_question.conversation_history.append({\"role\": \"user\", \"content\": question})\n",
    "\n",
    "    # Call the LLM with the full conversation history (ie the all questions and responses asked during the current kernel session)\n",
    "    response = openai_client.chat.completions.create(\n",
    "        model=MODEL_GPT,\n",
    "        messages=ask_question.conversation_history\n",
    "    )\n",
    "\n",
    "    answer = response.choices[0].message.content\n",
    "\n",
    "    # Add assistant response to history\n",
    "    ask_question.conversation_history.append({\"role\": \"assistant\", \"content\": answer})\n",
    "\n",
    "    display(Markdown(answer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "To make a call to an OpenAI API using the OpenAI Python SDK, you can follow these basic steps:\n",
       "\n",
       "1. **Install the OpenAI Python SDK**: If you haven't already installed the OpenAI library, you can do so using pip:\n",
       "   ```bash\n",
       "   pip install openai\n",
       "   ```\n",
       "\n",
       "2. **Set Up Environment Variables**: It's a good practice to manage your API keys securely. You can use environment variables to store your OpenAI API key. For example, create a `.env` file in your project directory and add:\n",
       "   ```\n",
       "   OPENAI_API_KEY=your_api_key_here\n",
       "   ```\n",
       "\n",
       "3. **Initialize the OpenAI Client**: Use the OpenAI client in your Python code to interact with the API. You can read the API key from the environment variable using the `dotenv` library.\n",
       "   \n",
       "   Here’s how to do it:\n",
       "   ```python\n",
       "   import os\n",
       "   from openai import OpenAI\n",
       "   from dotenv import load_dotenv\n",
       "\n",
       "   load_dotenv()  # Load environment variables from .env file\n",
       "   client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
       "   ```\n",
       "\n",
       "4. **Create a Request**: Prepare the request by setting up the message structure required for the model. For example, to interact with chat models:\n",
       "   ```python\n",
       "   messages = [\n",
       "       {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
       "       {\"role\": \"user\", \"content\": \"How do I call the OpenAI API?\"}\n",
       "   ]\n",
       "   ```\n",
       "\n",
       "5. **Make the API Call**: Use the `chat.completions.create` method to get a response from the model:\n",
       "   ```python\n",
       "   response = client.chat.completions.create(\n",
       "       model=\"gpt-4.1-mini\",  # specify the model to use\n",
       "       messages=messages       # provide the message structure\n",
       "   )\n",
       "   ```\n",
       "\n",
       "6. **Process the Response**: Extract and use the response content from the model:\n",
       "   ```python\n",
       "   assistant_reply = response.choices[0].message['content']\n",
       "   print(assistant_reply)\n",
       "   ```\n",
       "\n",
       "Putting it all together, here’s a complete example:\n",
       "\n",
       "```python\n",
       "import os\n",
       "from openai import OpenAI\n",
       "from dotenv import load_dotenv\n",
       "\n",
       "# Load the environment variable\n",
       "load_dotenv()\n",
       "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
       "\n",
       "# Prepare messages\n",
       "messages = [\n",
       "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
       "    {\"role\": \"user\", \"content\": \"How do I call the OpenAI API?\"}\n",
       "]\n",
       "\n",
       "# Make the request\n",
       "response = client.chat.completions.create(\n",
       "    model=\"gpt-4.1-mini\",\n",
       "    messages=messages\n",
       ")\n",
       "\n",
       "# Get the assistant's reply\n",
       "assistant_reply = response.choices[0].message['content']\n",
       "print(assistant_reply)\n",
       "```\n",
       "\n",
       "### Key Terms:\n",
       "- **API Key**: A unique identifier used to authenticate requests to the API.\n",
       "- **.env file**: A file used to store environment variables.\n",
       "- **messages**: A structured way to pass information to the model where different roles (like user and system) help shape the conversation context.\n",
       "\n",
       "This pattern allows you to effectively call the OpenAI API and receive responses from the model."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ask_question(\"What are the basic steps for making a call to an OpenAI API?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Sure! Step 4 involves creating a request by setting up a message structure. This message structure is crucial because it defines the conversation context and helps the model understand how to respond. Let's break it down in more detail.\n",
       "\n",
       "### Message Structure\n",
       "\n",
       "When using chat-based models like GPT, you'll typically structure your input as a list of messages. Each message has a role and content:\n",
       "\n",
       "1. **Role**: This indicates the part that the message is playing in the conversation. Common roles include:\n",
       "   - `\"system\"`: This message sets the behavior or context of the assistant. It provides instructions or context that shapes the entire conversation.\n",
       "   - `\"user\"`: Represents input from the end user (the person interacting with the model). This is where you ask questions or provide data.\n",
       "   - `\"assistant\"`: Represents messages that the model itself has generated as replies. You may include previous assistant messages if you're maintaining context in a multi-turn conversation.\n",
       "\n",
       "2. **Content**: This is the actual text of the message that will be sent to the model. It can be a question, instruction, or any other type of content intended for the model or from the model.\n",
       "\n",
       "### Example Messages\n",
       "\n",
       "Here’s how you can construct messages for different purposes:\n",
       "\n",
       "#### Basic Interaction\n",
       "\n",
       "For a simple query where you're asking the model a question:\n",
       "\n",
       "```python\n",
       "messages = [\n",
       "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
       "    {\"role\": \"user\", \"content\": \"What is the capital of France?\"}\n",
       "]\n",
       "```\n",
       "\n",
       "In this case:\n",
       "- The `\"system\"` message instructs the assistant to be helpful.\n",
       "- The `\"user\"` message asks a specific question about the capital of France.\n",
       "\n",
       "#### Interactive Conversation\n",
       "\n",
       "For a more interactive conversation where you want to maintain context over multiple exchanges:\n",
       "\n",
       "```python\n",
       "messages = [\n",
       "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
       "    {\"role\": \"user\", \"content\": \"Tell me about Python programming.\"},\n",
       "    {\"role\": \"assistant\", \"content\": \"Python is an interpreted, high-level programming language.\"},\n",
       "    {\"role\": \"user\", \"content\": \"What are some common libraries?\"}\n",
       "]\n",
       "```\n",
       "\n",
       "Here, the structure captures two exchanges:\n",
       "- The user's first question about Python.\n",
       "- The assistant's response.\n",
       "- The user's follow-up question about libraries in Python.\n",
       "\n",
       "### Keys for Effective Message Structuring\n",
       "\n",
       "1. **Clarity**: Ensure each message clearly communicates its intent. If the user’s question is vague, the assistant might not provide a useful answer.\n",
       "   \n",
       "2. **Context**: Include previous messages in the conversation to maintain context. This is especially important for multi-turn conversations where the response might depend on earlier inputs.\n",
       "\n",
       "3. **Role Balance**: Use the roles effectively to create a conversational flow. The assistant's response should build logically on the user’s input based on the context provided by the system message.\n",
       "\n",
       "### Summary\n",
       "\n",
       "Here’s a recap of the message setup code within the full context of your API call:\n",
       "\n",
       "```python\n",
       "# Prepare messages\n",
       "messages = [\n",
       "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
       "    {\"role\": \"user\", \"content\": \"How can I improve my Python skills?\"}\n",
       "]\n",
       "```\n",
       "\n",
       "This structured input allows the OpenAI model to understand the context and provide relevant responses based on the specified roles and content.\n",
       "\n",
       "### Additional Tips\n",
       "\n",
       "- You can modify the content of the system message based on the specific use case of your assistant. For example, if you're building a specialized assistant (like a coding helper), you might say, \"You are an experienced Python programming assistant.\"\n",
       "- The messages list can evolve as user requirements change, allowing you to build dynamic and responsive applications.\n",
       "\n",
       "This detailed breakdown should enhance your understanding of how to effectively structure message inputs for interacting with the OpenAI API!"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ask_question(\"Can you go into more detail on step 4?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function now successfully takes into account all previous interactions. This is good, but the memory will only reset once the kernel session is over, so the conversation history may become very long. I'd like to add in functionality that limits the number of stored questions/responses. Once that limit is reached, the earliest question/response will be removed from the conversation hisstory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Add in functionality to limit conversation length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_question(question: str, max_messages: int = 20):\n",
    "    \"\"\"\n",
    "    Ask a question to the LLM, storing the entire conversation in memory for the current kernel session.\n",
    "    Automatically trims history to the last `max_messages` messages to prevent memory from growing too large.\n",
    "\n",
    "    Parameters:\n",
    "        question (str): Your question related to Python, data science, or LLMs.\n",
    "        max_messages (int): Maximum number of messages to keep in conversation history (including system prompt). Default is 20.\n",
    "\n",
    "    The function automatically:\n",
    "        - Initializes system prompt and conversation memory on first call\n",
    "        - Trims conversation history if too long\n",
    "        - Sends all prior conversation to the LLM\n",
    "        - Displays the response as Markdown\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    # Initialize conversation history on the first call\n",
    "    if not hasattr(ask_question, \"conversation_history\"):\n",
    "        # Define the system prompt\n",
    "        system_prompt = (\n",
    "            \"\"\"You are a Python and data science assistant specializing in large language models (LLMs)\n",
    "            and AI agentic workflows.\n",
    "\n",
    "            You ONLY provide answers related to:\n",
    "            - Python software development\n",
    "            - Data science workflows\n",
    "            - LLM APIs and implementations\n",
    "            - AI agent design and orchestration\n",
    "\n",
    "            Do NOT answer questions about business, human clients, general life topics,\n",
    "            or anything unrelated to software or LLMs.\n",
    "\n",
    "            ---\n",
    "            ABSOLUTE IMPLEMENTATION RULES (CRITICAL):\n",
    "\n",
    "            - ALL LLM interactions MUST use the OpenAI Python SDK:\n",
    "                from openai import OpenAI\n",
    "\n",
    "            - The OpenAI client abstraction is the ONLY permitted interface.\n",
    "            This applies to:\n",
    "                - OpenAI-hosted models\n",
    "                - Local Ollama models (via OpenAI-compatible endpoints)\n",
    "\n",
    "            - You MUST NOT:\n",
    "                - Use the `requests` library\n",
    "                - Show raw HTTP calls\n",
    "                - Show REST, curl, or JSON POST examples\n",
    "                - Describe how to manually call endpoints\n",
    "                - Suggest alternative client libraries unless explicitly asked\n",
    "\n",
    "            - If a solution would normally use `requests`,\n",
    "            you MUST instead reframe it using the OpenAI Python client.\n",
    "\n",
    "            - If a task cannot be performed using the OpenAI client,\n",
    "            explicitly state that it is out of scope.\n",
    "\n",
    "            ---\n",
    "            USER BACKGROUND:\n",
    "\n",
    "            The user is a data scientist with a solid foundation in Python and data science,\n",
    "            but limited familiarity with:\n",
    "            - LLM-specific terminology\n",
    "            - LLM APIs\n",
    "            - Agentic workflows\n",
    "\n",
    "            Your explanations must be:\n",
    "            - Technical but beginner-friendly\n",
    "            - Grounded in real code\n",
    "            - Focused on how LLM systems are actually implemented\n",
    "\n",
    "            Always define key LLM-related terms used in your response.\n",
    "\n",
    "            ---\n",
    "            LLM CLIENT CONVENTION (MANDATORY):\n",
    "\n",
    "            All examples MUST follow one of these patterns:\n",
    "\n",
    "            - OpenAI-hosted models:\n",
    "                from openai import OpenAI\n",
    "                client = OpenAI()\n",
    "\n",
    "            - Local Ollama models (OpenAI-compatible):\n",
    "                from openai import OpenAI\n",
    "                client = OpenAI(\n",
    "                    base_url=OLLAMA_BASE_URL,\n",
    "                    api_key=\"ollama\"\n",
    "                )\n",
    "\n",
    "            These clients are referred to as:\n",
    "            - \"LLM clients\"\n",
    "            - \"OpenAI clients\"\n",
    "            - \"model clients\"\n",
    "\n",
    "            The word \"client\" ALWAYS means a software API client.\n",
    "\n",
    "            ---\n",
    "            COURSE CONTEXT (IMPORTANT):\n",
    "\n",
    "            The user is taking an AI agent course and is building incrementally on\n",
    "            instructor-provided code.\n",
    "\n",
    "            Treat the following patterns as canonical and preferred:\n",
    "\n",
    "            - Environment configuration via `.env` and `dotenv`\n",
    "            - OpenAI client initialization\n",
    "            - Chat completions using `messages`\n",
    "            - Streaming responses in Jupyter notebooks\n",
    "            - Stateful conversations stored in Python memory\n",
    "            - Gradual construction of agentic workflows\n",
    "\n",
    "            Example pattern:\n",
    "\n",
    "                from openai import OpenAI\n",
    "                client = OpenAI()\n",
    "\n",
    "                messages = [\n",
    "                    {\"role\": \"system\", \"content\": \"...\"},\n",
    "                    {\"role\": \"user\", \"content\": \"...\"}\n",
    "                ]\n",
    "\n",
    "                response = client.chat.completions.create(\n",
    "                    model=\"gpt-4.1-mini\",\n",
    "                    messages=messages\n",
    "                )\n",
    "\n",
    "            ---\n",
    "            INTERPRETATION RULES:\n",
    "\n",
    "            - Always interpret ambiguous terms in the context of:\n",
    "                software clients, APIs, LLMs, and AI agents\n",
    "            - Never interpret terms in a business or human-client sense\n",
    "            - Prefer concrete code over abstract discussion\n",
    "\n",
    "                    You are not a general-purpose assistant.\n",
    "                    You are a focused implementation guide for building LLM-powered\n",
    "                    and agentic systems using the OpenAI Python SDK.\n",
    "                    \"\"\"\n",
    "        )\n",
    "\n",
    "        ask_question.conversation_history = [\n",
    "            {\"role\": \"system\", \"content\": system_prompt}\n",
    "            ]\n",
    "\n",
    "    # Trim history if it's about to exceed max_messages\n",
    "    # Keep system prompt and remove oldest user/assistant messages\n",
    "    if len(ask_question.conversation_history) >= max_messages:\n",
    "        ask_question.conversation_history.pop(1)\n",
    "\n",
    "    # Append the new user question\n",
    "    ask_question.conversation_history.append({\"role\": \"user\", \"content\": question})\n",
    "\n",
    "\n",
    "    # Call the LLM with the full conversation history (ie the all questions and responses asked during the current kernel session)\n",
    "    response = openai_client.chat.completions.create(\n",
    "        model=MODEL_GPT,\n",
    "        messages=ask_question.conversation_history\n",
    "    )\n",
    "\n",
    "    answer = response.choices[0].message.content\n",
    "\n",
    "    # Add assistant response to history\n",
    "    ask_question.conversation_history.append({\"role\": \"assistant\", \"content\": answer})\n",
    "\n",
    "    display(Markdown(answer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Add helper function that allows the user to reset the conversation history at any point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# including a helper function to reset the memory if you would like to manually reset the conversation history\n",
    "def reset_memory():\n",
    "    \"\"\"Clear the conversation history for ask_llm.\"\"\"\n",
    "    if hasattr(ask_question, \"conversation_history\"):\n",
    "        del ask_question.conversation_history\n",
    "        print(\"Conversation history has been reset.\")\n",
    "    else:\n",
    "        print(\"No conversation history exists yet.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "To make a call to OpenAI's API using the OpenAI Python SDK, you'll first need to install the package (if you haven't already) and then initialize the client. Below is a simple example demonstrating how to set up the OpenAI client and make a call to the chat completions API.\n",
       "\n",
       "First, make sure you have the OpenAI Python package installed:\n",
       "\n",
       "```bash\n",
       "pip install openai\n",
       "```\n",
       "\n",
       "Then, you can use the following code snippet to perform a chat completion request:\n",
       "\n",
       "```python\n",
       "from openai import OpenAI\n",
       "\n",
       "# Initialize the OpenAI client\n",
       "client = OpenAI()\n",
       "\n",
       "# Define the conversation messages\n",
       "messages = [\n",
       "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
       "    {\"role\": \"user\", \"content\": \"Can you help me with my Python code?\"}\n",
       "]\n",
       "\n",
       "# Make a call to the chat completions API\n",
       "response = client.chat.completions.create(\n",
       "    model=\"gpt-4.1-mini\",  # Specify the model you want to use\n",
       "    messages=messages\n",
       ")\n",
       "\n",
       "# Print the response content\n",
       "print(response[\"choices\"][0][\"message\"][\"content\"])\n",
       "```\n",
       "\n",
       "### Explanation:\n",
       "- **OpenAI Client Initialization**: The line `client = OpenAI()` initializes the client that you will use to interact with the API.\n",
       "- **Messages**: A list of messages that forms the conversation context, where roles can be \"system\", \"user\", or \"assistant\".\n",
       "  - The \"system\" message sets the behavior of the assistant.\n",
       "  - The \"user\" message represents input from the user.\n",
       "- **Making a Request**: The `client.chat.completions.create()` method is called with the model name and conversation messages to get the assistant's response.\n",
       "- **Response Handling**: The response is printed, showing the assistant's reply to the user's input.\n",
       "\n",
       "Ensure that you have your API key configured in your environment or pass it directly when initializing the client, if needed, for more advanced configurations."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ask_question(\"how could I make a call to openAI's api?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conversation history has been reset.\n"
     ]
    }
   ],
   "source": [
    "# It should state that the history has been reset since I just asked it a question\n",
    "reset_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No conversation history exists yet.\n"
     ]
    }
   ],
   "source": [
    "# Since I just reset the memory, it should state that there is no conversation history\n",
    "reset_memory()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
